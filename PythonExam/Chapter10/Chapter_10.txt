第10章 网络爬虫和自动化

- The good news about computer is that they do what you tell them to
  do. The bad news is that they do what you tell them to do.  ———— Ted Nelson

- 学习目标
    1. 掌握网络爬虫的基本方法
    2. 运用requests库编写基本URL访问过程
    3. 运损beautifulsoup4库解析和处理HTML
    4. 掌握向搜索引擎自动提交关键词并获取返回结果的方法

- 随着网络的迅速发展，如何有效的提取并利用信息很大程度上决定了解决问题的效率。索索
  引擎作为辅助程序员检索信息的工具已经有些力不从心。为了更高效的获取指定信息需要定
  向抓取并分析网页资源，网络爬虫火爆了起来

10.1 问题概述
- 要点：Python语言实现网络爬虫的问题引入
- Python语言发展中有一个里程碑式的应用事件，即Google在搜索引擎后端采用Python
  进行链接处理和开发，这是该语言发展成熟的重要标志。
- Python语言的简洁性和脚本特点非常适合链接和网页处理，因此，在Python的计算生态
  中，与URL和网页处理相关的第三方库很多
- 万维网(www)的快速发展带来了大量获取和提交网络信息的需求，产生了网络爬虫等一系列
  应用。
    - Python语言提供了很多类似的函数库，包括：urllib, urllib2, urllib3,
      wget, scrapy, requests等
    - 对于爬取回来的网页内容，可以通过re(正则表达式)、beautifulsoup4等函数库
      来处理
- 网络爬虫应用一般分为两个步骤：
    1. 通过网络链接获取网页内容，可通过requests库
    2. 对获得的网页内容进行处理，可通过beautifulsoup4库
- 使用Python语言实现网络爬虫和信息提交是非常简单的事情，代码行数很少，也无须掌握
  网络通信等方面的知识，非常适合非专业读者使用。然而，肆意爬取网络数据并不是文明
  现象，通过程序自动提交内容争取竞争性资源也不公平。就像那些肆意的推销电话一样，无
  视接听者意愿，不仅令人讨厌也有可能引发法律纠纷
- 拓展：Robots排除协议
    - Robots Exclusion Protocol, 也被称为爬虫协议，它是网站管理者表达是否
      希望爬虫自动获取网络信息意愿的方法。
    - 管理者可以在网站根目录放置一个robots.txt文件，并在文件中列出哪些链接不允
      许爬虫爬取。一般搜索引擎的爬虫会首先捕获这个文件，并根据文件要求爬取网站内
      容。
    - 排除协议重点约定不希望爬虫获取的内容，如果没有该文件则表示网站内容可以被爬
      虫获得，然而，Robots协议不是命令和强制手段，只是国际互联网的一种通用道德
      规范。绝大部分成熟的搜索引擎爬虫会遵循这个协议，建议个人也能按照互联网规范
      要求合理使用爬虫技术
- 思考与练习
    - 10.1 请思考网络爬虫的可能应用
        - 可通过大量获取网页内容来为数据挖掘和机器学习提供训练数据

10.2 模块10：requests库的使用
- 要点：requests库是一个简洁且简单的处理HTTP请求的第三方库
- 10.2.1 requests库概述
    - requests库最大的优点是程序编写过程更接近正常URL访问过程
    - requests库建立在urllib3库的基础上
        - 类似这种在其他函数库之上再封装功能、提供更友好函数的方式在Python语言
          中十分常见。
        - 在Python生态圈里，任何人都有通过技术创新或体验创新发表意见和展示才华
          的机会
    - requests库支持非常丰富的链接访问功能，包括：
        - 国际域名和URL获取
        - HTTP长连接和连接缓存
        - HTTP会话和Cookie保持
        - 浏览器使用风格的SSL验证
        - 基本的摘要认证
        - 有效的键值对Cookie记录
        - 自动解压缩
        - 自动内容解码
        - 文件分块上传
        - HTTP(S)代理功能
        - 连接超时处理
        - 流数据下载
        - …
    - 更多介绍：http://docs.python-requests.org
-10.2.2 requests库解析
    - 6个网页请求函数
        - get(url[, timeout=n])
            - 对应于HTTP的GET方式，获取网页最常用的方法，可以增加timeout=n
              参数，设定每次请求超时时间为n秒
            - 获取网页最常用的方式，调用后返回的网页内容会保存为一个Response
              对象
            - r = requests.get('http://shunz.cc')
        - post(url, data={'key':'value'})
            - 对应于HTTP的POST方式，其中字典用于传递客户数据
        - delete(url)
            - 对应于HTTP的DELETE方式
        - head(url)
            - 对应于HTTP的HEAD方式
        - options(url)
            - 对应于HTTP的OPTIONS方式
        - put(url, data={'key':'value'})
            - 对应于HTTP的PUT方式，其中字典用于传递客户数据
    - 4个Response对象的属性
        - status_code
            - HTTP请求的返回状态，整数，200表示连接成功，404表示失败
            - 在处理数据之前要先判断状态情况，如果请求未被响应，需要终止内容处理
        - text
            - HTTP响应内容的字符串形式，即url对应的页面内容
        - encoding
            - HTTP响应内容的编码方式
            - 非常重要，可通过对其赋值更改编码方式，以便处理中文字符
        - content
            - HTTP响应内容的二进制形式
    - 2个Response对象的方法
        - json()
            - 如果HTTP响应内容包含JSON格式数据，则该方法解析JSON数据
        - raise_for_status()
            - 如果不是200，则产生异常，用于try-except语句
            - 可以避免设置一堆复杂的if语句，只需在收到响应时调用此方法，即可避
              开状态字200以外的各种意外情况
            - 常见异常
                - 遇到网络问题，如DNS查询失败、拒绝连接等
                    - 抛出ConnectionError异常
                - 遇到无效HTTP响应时
                    - 抛出HTTPError异常
                - 请求url超时时
                    - 抛出Timeout异常
                - 请求超过设定的最大重定向次数
                    - 抛出一个TooManyRedirects异常
    - 获取一个网页内容的建议函数
        import requests
        def getHTMLText(url):
            try:
                r = requests.get(url, timeout=30)
                r.raise_for_status()  # 如果状态不是200，引发异常
                r.encoding = 'utf-8'  # 无论原来用什么编码，都改成utf-8
                return r.text
            except:
                return ''
        url = 'http://www.baidu.com'
        print(getHTMLText(url))
    - 拓展：HTTP的GET和POST
        - HTTP协议定义了客户端与服务器交互的不同方法，最基本的就是GET和POST
        - POST用于发送内容
        - GET可以根据某链接获得内容，同时，也可向链接提交内容，与POST区别如下：
            1. GET方式可以通过URL提交数据，待提交数据是URL的一部分。采用POST
               方式的待提交数据放置在HTML HEADER内
            2. GET方式提交的数据最多不超过1024字节，POST没有对提交内容的长度
               限制
            3. 安全性问题。使用GET时参数会显示在URL中，而POST不会，所以，如果
               数据是非敏感数据，那么使用GET，否则，建议采用POST方式
              

10.3 模块11：beautifulsoup4库的使用
- 要点：beautifulsoup4是一个解析和处理HTML和XML的第三方库
- 10.3.1 beautifulsoup4库概述
    - 简称：bs4
    - 最大优点：能根据HTML和XML语法建立解析树，进而高效解析其中的内容
    - HTML建立的Web页面一般非常复杂，除了有用的内容信息外，还包括大量用于页面格
      式的元素，直接解析一个Web页面需要深入了解HTML语法，而且比较复杂。bs4库将专业
      业的Web页面格式解析部分封装成函数，提供了若干有用且便捷的处理函数
    - bs4库采用面向对象思想实现，把每个页面当做一个对象，通过<a>.<b>的方式调用
      对象的属性(即包含的内容),或通过<a>.<b>()的方式调用方法(即处理函数)
    - 由于库名字很特殊且采用面向对象方式组织，可以用from-import方式直接引用
        - from bs4 import BeautifulSoup
    - 关于bs4库的更多介绍
        - https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/
- 10.3.2 bs4库解析
    - bs4库中最主要的是BeautifulSoup类，每个实例化的对象相当于一个页面
    - 采用from-import导入库中的BeautifulSoup类后，使用BeautifulSoup()
      创建一个BeautifulSoup对象
    - 创建的BeautifulSoup对象是一个树形结构，包含HTML页面中的每一个Tag元素
        - 具体来说，HTML中的主要结构都变成了BS对象的一个属性，可以直接用
          a.b形式获得，其中b的名字采用HTML中标签的名字
    
    

10.4 实例20：中国大学排名爬虫


10.5 实例21：搜索关键词自动提交


本章小结
